<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>N2V - ImageJ</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"N2V","wgTitle":"N2V","wgCurRevisionId":45826,"wgRevisionId":45826,"wgArticleId":12723,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"N2V","wgRelevantArticleId":12723,"wgRequestId":"ef7a80f199da88a91af6a614","wgIsProbablyEditable":false,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgPreferredVariant":"en","fancytree_path":"/extensions/TreeAndMenu/fancytree"});mw.loader.state({"site.styles":"ready","noscript":"ready","user.styles":"ready","user.cssprefs":"ready","user":"ready","user.options":"loading","user.tokens":"loading","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","skins.erudite":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1ku9xth",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["mediawiki.page.startup"]);});</script>
<link rel="stylesheet" href="erudite14.css"/>
<link rel="stylesheet" href="extensions/TreeAndMenu/fancytree/fancytree.css"/><link rel="stylesheet" href="extensions/TreeAndMenu/suckerfish/suckerfish.css"/>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="erudite15.css"/>
<meta name="generator" content="MediaWiki 1.28.0"/>
<meta name="description" content="Noise2Void (N2V) is a powerful, context aware and flexible algorithm for image denoising.&#10;It uses artificial neural networks to learn about the properties of your images and how to best denoise them.&#10;N2V outperforms traditional denoising techniques."/>
<link rel="shortcut icon" href="skins/ij2.ico"/>
	<meta property="og:type" content="article"/>

	<meta property="og:site_name" content="ImageJ"/>

	<meta property="og:title" content="N2V"/>

	<meta property="og:description" content="Noise2Void (N2V) is a powerful, context aware and flexible algorithm for image denoising.&#10;It uses artificial neural networks to learn about the properties of your images and how to best denoise them.&#10;N2V outperforms traditional denoising techniques."/>


<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-N2V rootpage-N2V skin-erudite action-view">
		<div id="top-wrap" role="banner">
			<h1><a href="Welcome" title="ImageJ" rel="home">ImageJ</a></h1>
			<div id="tagline">From ImageJ</div>

			<a id="menubutton" href="N2V.html#menu">Menu</a>
			<div id="nav" role="navigation">
			<ul id='menu'>
<li id="menu-item-n-About"><a href="ImageJ">About</a></li>
<li id="menu-item-n-Downloads"><a href="Downloads">Downloads</a></li>
<li id="menu-item-n-Learn"><a href="Learn">Learn</a></li>
<li id="menu-item-n-Develop"><a href="Development">Develop</a></li>
<li id="menu-item-n-News"><a href="News">News</a></li>
<li id="menu-item-n-Events"><a href="Events">Events</a></li>
<li id="menu-item-n-Help"><a href="Help">Help</a></li>
</ul>
			</div>
		</div>

		<div id="mw-js-message"></div>
		
		<div id="main" role="main">

			<div id="bodyContent">
				<div style="font-size: large; border: 1px solid black; padding: 1em; margin-bottom: 1em; text-align: center; background-color: #fda;">
					This is a read-only version of imagej.net, available during the transition to the new website, which you can preview <a href="https://imagej.github.io/">here</a>.
					<br>Please direct any questions or issues to <a href="https://forum.image.sc/t/imagej-wiki-is-down/39672">this Image.sc Forum thread</a>.
					<br>Thank you for your patience as we improve the website!
				</div>

				<h1>N2V</h1>
				
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-teaser.png" class="image"><img alt="" src="_images/6/6c/N2v-teaser.png" width="300" height="256" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-teaser.png" class="internal" title="Enlarge"></a></div>Examples of N2V denoised images.</div></div></div>
<p>Noise2Void (N2V) is a powerful, context aware and flexible algorithm for image denoising.
It uses artificial neural networks to learn about the properties of your images and how to best denoise them.
N2V outperforms traditional denoising techniques.
</p><p>This page describes the N2V FIJI plugin. The N2V FIJI plugin provides a very simple way to use N2V in FIJI.
All you need is a computer with a NVIDIA graphics cards, a FIJI installation and your noisy images.
</p><p>The execution of N2V has two steps.
The first step will train the artificial neural network to remove the noise in the kind of images that you have.
This training step is relatively slow, it might take around 12 h to get the best results.
But don't worry. Just let your computer run the training over night. You don't need to do anything. Training only needs to happen once, and you will see a preview during the ongoing training.
The result of the training is a model.
The second step is called prediction. The prediction step will use the trained model to denoise your images. The same model can be used for any number of similar images. It typically takes less than a second per image. So high quality denoising of thousand images is easily possible within one day.
</p><p>See the paper for a detailed description of the algorithm.
</p><p>This FIJI plugin is part of CSBDeep. A set of open source neural network algorithms in FIJI. For more information, examples and images, click <a rel="nofollow" class="external text" href="https://csbdeep.bioimagecomputing.com/tools/n2v/">here</a>.
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="./N2V.html#Publication:_Noise2Void_-_Learning_Denoising_from_Single_Noisy_Images"><span class="tocnumber">1</span> <span class="toctext">Publication: Noise2Void - Learning Denoising from Single Noisy Images</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="N2V.html#Installation"><span class="tocnumber">2</span> <span class="toctext">Installation</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="N2V.html#Usage"><span class="tocnumber">3</span> <span class="toctext">Usage</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="N2V.html#Training"><span class="tocnumber">3.1</span> <span class="toctext">Training</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="N2V.html#Training_on_a_single_image"><span class="tocnumber">3.1.1</span> <span class="toctext">Training on a single image</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="N2V.html#Training_and_prediction_on_single_images_.28one-click_solution.29"><span class="tocnumber">3.1.2</span> <span class="toctext">Training and prediction on single images (one-click solution)</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="N2V.html#Training_on_multiple_images"><span class="tocnumber">3.1.3</span> <span class="toctext">Training on multiple images</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="N2V.html#What_happens_during_and_after_training"><span class="tocnumber">3.2</span> <span class="toctext">What happens during and after training</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="N2V.html#Prediction"><span class="tocnumber">3.3</span> <span class="toctext">Prediction</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="N2V.html#Exporting_trained_models_from_Python_to_ImageJ_.2F_Fiji"><span class="tocnumber">4</span> <span class="toctext">Exporting trained models from Python to ImageJ / Fiji</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="N2V.html#How_to_handle_macros_.2F_scripts_.2F_models_from_the_first_early_release_of_N2V_for_Fiji"><span class="tocnumber">5</span> <span class="toctext">How to handle macros / scripts / models from the first early release of N2V for Fiji</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="N2V.html#Update_Site"><span class="tocnumber">5.1</span> <span class="toctext">Update Site</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="N2V.html#Macros_.2F_Scripts"><span class="tocnumber">5.2</span> <span class="toctext">Macros / Scripts</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="N2V.html#Trained_models"><span class="tocnumber">5.3</span> <span class="toctext">Trained models</span></a></li>
</ul>
</li>
</ul>
</div>

<h1><span class="mw-headline" id="Publication:_Noise2Void_-_Learning_Denoising_from_Single_Noisy_Images">Publication: Noise2Void - Learning Denoising from Single Noisy Images</span></h1>
<p><b>Abstract</b>
</p><p>The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.
</p><p><b><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1811.10980">Full-text</a></b>
</p>
<h1><span class="mw-headline" id="Installation">Installation</span></h1>
<ol><li> Start ImageJ / Fiji</li>
<li> Open the updater via <code>Help &gt; Update...</code></li>
<li> Click on <code>Manage update sites</code></li>
<li> Select the <b><code>CSBDeep</code></b> update site</li>
<li> Click on <code>Apply changes</code></li>
<li> (optional) read <a rel="nofollow" class="external text" href="TensorFlow-GPU">this page</a> for GPU support</li>
<li> Restart ImageJ / Fiji</li></ol>
<p>You should now have access to these plugins:
</p>
<img src="_images/n2v/csb_deep_plugin.png", width="300", height="100">
<h1><span class="mw-headline" id="Usage">Usage</span></h1>
<h2><span class="mw-headline" id="Training">Training</span></h2>
<p>Training without GPU support is possible, but will take ages. Please read the notes on <a rel="nofollow" class="external text" href="TensorFlow-GPU">this page</a> for how to run the tools on the GPU.
</p>
<h3><span class="mw-headline" id="Training_on_a_single_image">Training on a single image</span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-train-parameters.png" class="image"><img alt="" src="_images/0/00/N2v-train-parameters.png" width="300" height="245" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-train-parameters.png" class="internal" title="Enlarge"></a></div>N2V train parameters</div></div></div>
<ol><li> Start ImageJ / Fiji</li>
<li> Open a noisy image of your choice (it should be sufficiently large)</li>
<li> (optional) open another noisy image for validation (judging how well the training is performing)</li>
<li> Click on <code>Plugins &gt; CSBDeep &gt; N2V &gt; N2V train</code> and adjust the following parameters:
<ul><li> <b><code>Image used for training</code></b> Choose the image which will be used for training</li>
<li> <b><code>Image used for validation</code></b> Choose the image which will be used for training (you can also choose the same for both images, in this case 10% of the tiled image will be used for validation and 90% for training)</li>
<li> <b><code>Use 3D model instead of 2D</code></b> Select this checkbox if you want to train on 3D data (this needs much more GPU memory)</li>
<li> <b><code>Number of epochs</code></b> How many epochs should be performed during training</li>
<li> <b><code>Number of steps per epoch</code></b> How many steps per epoch should be performed</li>
<li> <b><code>Batch size per step</code></b> How many tiles are batch processed by the network per training step</li>
<li> <b><code>Patch shape</code></b> The length of X, Y (and Z) of one training patch (needs to be a multiple of 16)</li>
<li> <b><code>Neighborhood radius</code></b> n2V specific parameter describing the distance of the neighbor pixel replacing the center pixel</li></ul></li>
<li> Click <code>Ok</code></li>
<li> Look below at the <a href="N2V.html#What_happens_during_and_after_training">What happens during and after training</a> section for what happens next</li></ol>
<h3><span class="mw-headline" id="Training_and_prediction_on_single_images_.28one-click_solution.29">Training and prediction on single images (one-click solution)</span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-trainpredict-parameters.png" class="image"><img alt="" src="_images/8/8b/N2v-trainpredict-parameters.png" width="300" height="197" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-trainpredict-parameters.png" class="internal" title="Enlarge"></a></div>N2V train &amp; predict parameters</div></div></div>
<ol><li> Start ImageJ / Fiji</li>
<li> Open a noisy image of your choice (it should be sufficiently large)</li>
<li> Open another noisy image you want to denoise directly after training (this will also be used for validation)</li>
<li> Click on <code>Plugins &gt; CSBDeep &gt; N2V &gt; N2V train &amp; predict</code> and adjust the following parameters:
<ul><li> <b><code>Image used for training</code></b> Choose the image which will be used for training</li>
<li> <b><code>Image to denoise after training</code></b> Choose the image which will be used for prediction</li>
<li> <b><code>Axes of prediction input</code></b> This parameter helps to figure out how your input data is organized. It's a string with one letter per dimension of the input image. For 2D images, this should be <code>XY</code>. If your data has another axis which should be batch processed, set this parameter to <code>XYB</code></li>
<li> Regarding the other parameters please have a look at the descriptions in <a href="N2V.html#Training_on_a_single_image">Training on a single image</a></li></ul></li>
<li> Click <code>Ok</code></li>
<li> Look below at the <a href="N2V.html#What_happens_during_and_after_training">What happens during and after training</a> section for what happens next</li></ol>
<h3><span class="mw-headline" id="Training_on_multiple_images">Training on multiple images</span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-trainfolder-parameters.png" class="image"><img alt="" src="_images/4/4c/N2v-trainfolder-parameters.png" width="300" height="191" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-trainfolder-parameters.png" class="internal" title="Enlarge"></a></div>N2V train on folder parameters</div></div></div>
<ol><li> Start ImageJ / Fiji</li>
<li> Click on <code>Plugins &gt; CSBDeep &gt; N2V &gt; N2V train on folder</code> and adjust the following parameters:
<ul><li> <b><code>Folder containing images used for training</code></b> Choose the folder containing images which should be used for training</li>
<li> <b><code>Folder containing images used for validation</code></b> Choose the folder containing images which should be used for validation (can be same as training folder, in this case 10% of the generated tiles will be used for validation and 90% for training)</li>
<li> Regarding the other parameters please have a look at the descriptions in <a href="N2V.html#Training_on_a_single_image">Training on a single image</a></li></ul></li>
<li> Click <code>Ok</code></li>
<li> Look below at the <a href="N2V.html#What_happens_during_and_after_training">What happens during and after training</a> section for what happens next</li></ol>
<h2><span class="mw-headline" id="What_happens_during_and_after_training">What happens during and after training</span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-train-progress.png" class="image"><img alt="" src="_images/9/9f/N2v-train-progress.png" width="300" height="302" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-train-progress.png" class="internal" title="Enlarge"></a></div>N2V training progress window</div></div></div>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-train-preview.png" class="image"><img alt="" src="_images/0/06/N2v-train-preview.png" width="300" height="323" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-train-preview.png" class="internal" title="Enlarge"></a></div>N2V training preview window</div></div></div>
<p>During training, you will see two windows:
</p>
<ul><li> The progress window keeps you updated of the steps the training process is going through. It also plots the current training and validation loss. </li>
<li> The preview window is generated from the first validation batch. It is slit into two parts. The upper left part displays the original noisy data, the lower right part displays the prediction at the current state of the training. </li></ul>
<p>After training, two additional windows should appear. They represent two trained models. One is the model from the epoch with the lowest validation loss, the other one the model from the last epoch step. For N2V, using the model from the last epoch is almost always recommended. The windows will look similar to this:
</p>
<div class="MediaTransformError" style="width: 602px; height: 0px; display:inline-block;">Error creating thumbnail: Unable to save thumbnail to destination</div>
<p>They are stored to a temporary location which you can see in the Overview section of the model window under <code>Saved to..</code>. 
</p><p><b>Copy the model from there to another permanent destination on your disk if you want to keep this trained model.</b>
</p>
<h2><span class="mw-headline" id="Prediction">Prediction</span></h2>
<p>There are two ways to predict from a trained model.
</p><p>You can <b>open the model directly</b>:
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-modelpredict-parameters.png" class="image"><img alt="" src="_images/8/86/N2v-modelpredict-parameters.png" width="300" height="65" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-modelpredict-parameters.png" class="internal" title="Enlarge"></a></div>N2V prediction from model parameters</div></div></div>
<ol><li> Start Fiji</li>
<li> Open an image you want to denoise and for which you have a pretrained model available as ZIP file</li>
<li> Click <code>Import &gt; bioimage.io.zip</code> and choose your trained model. The model will open in a window as depicted above</li>
<li> Click <code>Predict</code> in the model window and adjust the following parameters:
<ul><li> <b><code>Input</code></b> The image you want to denoise</li>
<li> <b><code>Axes of prediction input</code></b> This parameter helps to figure out how your input data is organized. It's a string with one letter per dimension of the input image. For 2D images, this should be <code>XY</code>. If your data has another axis which should be batch processed, set this parameter to <code>XYB</code></li></ul></li></ol>
<p>Alternatively, you can <b>use the N2V menu</b>:
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="./File:N2v-predict-parameters.png" class="image"><img alt="" src="_images/9/95/N2v-predict-parameters.png" width="300" height="80" class="thumbimage" /></a>  <div class="thumbcaption"><div class="magnify"><a href="./File:N2v-predict-parameters.png" class="internal" title="Enlarge"></a></div>N2V prediction parameters</div></div></div>
<ol><li> Start Fiji</li>
<li> Open an image you want to denoise and for which you have a pretrained model available as ZIP file</li>
<li> Click <code>Plugins &gt; N2V &gt; N2V predict</code> and adjust the parameters as described above, with this addition:
<ul><li> <b><code>Trained model file</code></b> The ZIP file containing the pretrained model (it should end with <code>.bioimage.io.zip</code>)</li></ul></li></ol>
<h1><span class="mw-headline" id="Exporting_trained_models_from_Python_to_ImageJ_.2F_Fiji">Exporting trained models from Python to ImageJ / Fiji</span></h1>
<p>It's possible to train a Noise2Void neural network using Python. The required code and instructions can be found <a rel="nofollow" class="external text" href="https://github.com/juglab/n2v">here</a>. The model that has been trained in Python, can be used in FIJI as well: 
</p>
<ol><li> In Python, run this at the end of you training: <code>model.export_TF()</code></li>
<li> Locate the exported model file</li>
<li> Proceed as described in <a href="N2V.html#Prediction">Prediction</a></li></ol>
<h1><span class="mw-headline" id="How_to_handle_macros_.2F_scripts_.2F_models_from_the_first_early_release_of_N2V_for_Fiji">How to handle macros / scripts / models from the first early release of N2V for Fiji</span></h1>
<p>Thank you for testing the first early release version! Here is what changed, if that does not help you getting already trained models or scripts running, please write a post in the forum!
</p>
<h2><span class="mw-headline" id="Update_Site">Update Site</span></h2>
<p>You don't need the N2V update site any more, the CSBDeep update site is sufficient. Please remove the N2V update site.
</p>
<h2><span class="mw-headline" id="Macros_.2F_Scripts">Macros / Scripts</span></h2>
<ul><li> the <code>predict</code> command was renamed to <code>N2V predict</code></li>
<li> the <code>train</code> command was renamed to <code>N2V train</code></li>
<li> the <code>train + predict</code> command was renamed to <code>N2V train + predict</code></li>
<li> the <code>train on folder</code> command was renamed to <code>N2V train on folder</code></li>
<li> there is a new mandatory prediction parameter called <code>axes</code> (see documentation above)</li>
<li> the training parameter <code>batchDimLength</code> is gone for good</li>
<li> the training parameter <code>patchDimLength</code> was renamed to <code>patchShape</code></li>
<li> the training output <code>latestTrainedModelPath</code> changed to <code>latestTrainedModel</code> (it's a displayable model object now)</li>
<li> the training output <code>bestTrainedModelPath</code> changed to <code>bestTrainedModel</code> (it's a displayable model object now)</li></ul>
<h2><span class="mw-headline" id="Trained_models">Trained models</span></h2>
<p>Models trained with the first N2V for Fiji version cannot be used with the newer commands for model prediction. Please upgrade them first by using the command <code>Plugins &gt; CSBDeep &gt; N2V &gt; Upgrade old N2V model</code>.
! Note: When testing this, I had to unzip and zip the new model before it was usable. I'll try to fix this, but if you run into problems with the converted model, try unzipping and zipping it again.
</p>
<!-- 
NewPP limit report
Cached time: 20200713073033
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.040 seconds
Real time usage: 0.043 seconds
Preprocessor visited node count: 55/1000000
Preprocessor generated node count: 60/1000000
Postâ€expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/3
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 - -total
-->
</div><div class="printfooter">
Retrieved from "<a dir="ltr" href="index.php?title=N2V&amp;oldid=45826">http://imagej.net/index.php?title=N2V&amp;oldid=45826</a>"</div>
							</div>

			<div id="footer">
				<p> This page was last modified on 25 June 2020, at 07:07.</p><ul><li><a href="./ImageJ:Privacy_policy" title="ImageJ:Privacy policy">Privacy policy</a></li><li><a href="./ImageJ:About" class="mw-redirect" title="ImageJ:About">About ImageJ</a></li><li><a href="Imprint" title="Imprint">Imprint</a></li></ul>			</div>

			<div id="catlinks" class="catlinks catlinks-allhidden" data-mw="interface"></div>		</div>

		<div id="bottom-wrap">
			<div id="footer-wrap-inner">
				<div id="ternary" class="footer">
					<ul>
						<li class="widget">
							<img id="logo" src="skins/imagej-128.png" alt="">
						</li>
					</ul>
				</div>
			</div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.loader.load(["ext.fancytree","ext.suckerfish","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.SimpleTooltip"]);});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":247});});</script>
		</body>
		</html>
		